{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMu3D/JE+L8ZHCsS3CDPSJU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"erSSG0LQKkTN"},"source":["#Load Libraries"]},{"cell_type":"code","metadata":{"id":"DjrEMeL66Qdp"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import spacy\n","import numpy as np\n","\n","import os\n","import time\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVSdmHl26Vf2"},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vNCPa42DOMzt"},"source":["#Load CSV File to train the models on"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1xykpi16Y7F","executionInfo":{"status":"ok","timestamp":1608493385922,"user_tz":-330,"elapsed":27360,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"f800c6e2-d16c-4898-9a07-edb771afee7f"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"wtgej4_MvW8a","executionInfo":{"status":"ok","timestamp":1608486375635,"user_tz":-330,"elapsed":2828,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"2b81dcc0-3f08-41c8-faaf-7ea4f10b4ef3"},"source":["import numpy as np\n","import pandas as pd\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","path = \"/content/drive/My Drive/project/\"\n","training_dataset = pd.read_csv(path + 'em.csv')\n","\n","training_dataset.columns = ['nfr','mono','di','tri']\n","training_dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>nfr</th>\n","      <th>mono</th>\n","      <th>di</th>\n","      <th>tri</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.148752</td>\n","      <td>0.190306</td>\n","      <td>2.152600</td>\n","      <td>0.088854</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.148752</td>\n","      <td>0.190306</td>\n","      <td>2.152600</td>\n","      <td>0.088854</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.148752</td>\n","      <td>0.190306</td>\n","      <td>2.152600</td>\n","      <td>0.088854</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.148752</td>\n","      <td>0.190306</td>\n","      <td>2.152600</td>\n","      <td>0.088854</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.222059</td>\n","      <td>1.638257</td>\n","      <td>2.155221</td>\n","      <td>0.088854</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        nfr      mono        di       tri\n","0  2.148752  0.190306  2.152600  0.088854\n","1  2.148752  0.190306  2.152600  0.088854\n","2  2.148752  0.190306  2.152600  0.088854\n","3  2.148752  0.190306  2.152600  0.088854\n","4  1.222059  1.638257  2.155221  0.088854"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"id":"bFE5F8BSviNm"},"source":["from time import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from sklearn.cluster import KMeans\n","from sklearn.datasets import load_digits\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import scale"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3x7IcZjXOVbL"},"source":["##Estimate State Labels by using K-Means Directly"]},{"cell_type":"code","metadata":{"id":"glI8GbVgvmDR"},"source":["n_samples, n_features = training_dataset.shape\n","n_classes = 3\n","estimator = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)\n","estimator.fit(training_dataset)\n","labels = estimator.predict(training_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_BGAzYbvoVz","executionInfo":{"status":"ok","timestamp":1608486382343,"user_tz":-330,"elapsed":5197,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"7d51b4de-ae19-4e1c-d286-df63832f7502"},"source":["estimator.cluster_centers_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.19730298, 0.08590303, 0.32614811, 0.05594202],\n","       [1.2451336 , 2.93929402, 2.42497628, 0.32360281],\n","       [1.08004871, 2.63965522, 0.22601982, 0.05596694]])"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"w7KBYmPSvqA3","executionInfo":{"status":"ok","timestamp":1608486382344,"user_tz":-330,"elapsed":4828,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"27e62918-f0d7-4382-e8a3-f6c2ddf5b5e4"},"source":["labels = pd.DataFrame(labels)\n","labels.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   0\n","0  0\n","1  0\n","2  0\n","3  0\n","4  1"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ir92DOvZQtzV","executionInfo":{"status":"ok","timestamp":1608486752966,"user_tz":-330,"elapsed":1082,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"f6b04b22-6989-4dfd-c692-e8eacd5ff9fd"},"source":["nfr_values = np.array([estimator.cluster_centers_[i][0] for i in range(3)])\n","index_C = np.argmax(nfr_values)\n","index_B = np.argmin(nfr_values)\n","index_L = 3 - index_C - index_B\n","print(nfr_values)\n","print(index_L)\n","print(index_B)\n","print(index_C)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.19730298 1.2451336  1.08004871]\n","2\n","0\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"MbwX-3I6vwqT","executionInfo":{"status":"ok","timestamp":1608486796131,"user_tz":-330,"elapsed":1009,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"98e500d8-e872-4ffc-cb36-6608e8b261f0"},"source":["labels = labels.replace({index_C:'C', index_B:'B', index_L:'L'})\n","# labels = pd.get_dummies(labels[labels.columns[0]])\n","labels.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   0\n","0  A\n","1  A\n","2  A\n","3  A\n","4  B"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"code","metadata":{"id":"qunWvPFNv-L7"},"source":["training_dataset.head()\r\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tSc5uTydZx5F"},"source":["#Univariate LSTM"]},{"cell_type":"markdown","metadata":{"id":"5EqpWQrOPLcn"},"source":["##Training LSTM Models"]},{"cell_type":"markdown","metadata":{"id":"Ajo9_pGcOeRu"},"source":["###Train Univariate LSTM for each of the columns"]},{"cell_type":"code","metadata":{"id":"sEQ3F0IBnolR"},"source":["training_dataset.head()\n","\n","''' The user needs to train the LSTM for each of the columns in the dataset.\n","    This is made possible by setting the to_do value to the particular column name.\n","    For example, here to_do = 'nfr' trains an LSTM over the nfr column values.\n","    If you want to train an LSTM for each of the columns, simply change the value of to_do (uncomment the other line and comment the current one)\n","'''\n","\n","to_do = 'nfr'\n","# to_do = 'mono'\n","# to_do = 'di'\n","# to_do = 'tri'\n","\n","from sklearn.preprocessing import MinMaxScaler\n","price = training_dataset[[to_do]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"caIoRHi9RU56","executionInfo":{"status":"ok","timestamp":1608478849019,"user_tz":-330,"elapsed":1062,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"8b6dff60-ec13-4b87-ebf8-115405922d6b"},"source":["price.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tri</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.088854</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.088854</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.088854</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.088854</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.088854</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        tri\n","0  0.088854\n","1  0.088854\n","2  0.088854\n","3  0.088854\n","4  0.088854"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"meHJKNvGPxkw"},"source":["###Split into Train and Validation Set (Default 80-20 ratio) and Create DataLoaders"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"leVHOP4fRb_0","executionInfo":{"status":"ok","timestamp":1608478852114,"user_tz":-330,"elapsed":2726,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"804cf907-3ed8-4f0e-a057-d39613fa33ae"},"source":["''' Change the Train-Validation split ratio here '''\n","ratio = 0.2\n","\n","def split_data(stock, lookback):\n","    data_raw = stock.to_numpy() # convert to numpy array\n","    data = []\n","    \n","    # create all possible sequences of length seq_len\n","    for index in range(len(data_raw) - lookback): \n","        data.append(data_raw[index: index + lookback])\n","    \n","    data = np.array(data);\n","    test_set_size = int(np.round(ratio*data.shape[0]));\n","    train_set_size = data.shape[0] - (test_set_size);\n","    \n","    x_train = data[:train_set_size,:-1,:]\n","    y_train = data[:train_set_size,-1,:]\n","    \n","    x_test = data[train_set_size:,:-1]\n","    y_test = data[train_set_size:,-1,:]\n","    \n","    return [x_train, y_train, x_test, y_test]\n","\n","''' Set the context window size. Default is a context size of 20 samples.\n","    This determines how much of the previous data points you are exposing to the LSTM Model.\n","    E.g., in this case, you expose 20 sequential samples and then predict the 21st value.\n","'''\n","\n","lookback = 20 # choose sequence length\n","\n","x_train, y_train, x_test, y_test = split_data(price, lookback)\n","\n","print('x_train.shape = ',x_train.shape)\n","print('y_train.shape = ',y_train.shape)\n","print('x_test.shape = ',x_test.shape)\n","print('y_test.shape = ',y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x_train.shape =  (801502, 19, 1)\n","y_train.shape =  (801502, 1)\n","x_test.shape =  (200375, 19, 1)\n","y_test.shape =  (200375, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"771p5nqCZpt4"},"source":["# make training and test sets in torch\n","x_train = torch.from_numpy(x_train).type(torch.Tensor)\n","x_test = torch.from_numpy(x_test).type(torch.Tensor)\n","y_train = torch.from_numpy(y_train).type(torch.Tensor)\n","y_test = torch.from_numpy(y_test).type(torch.Tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhJttH8jZZ5a"},"source":["n_steps = lookback-1\n","batch_size = 1606\n","#n_iters = 3000\n","num_epochs = 50 #n_iters / (len(train_X) / batch_size)\n","\n","train = torch.utils.data.TensorDataset(x_train,y_train)\n","test = torch.utils.data.TensorDataset(x_test,y_test)\n","\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train, \n","                                           batch_size=batch_size, \n","                                           shuffle=False)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyhYeBOlQJxo"},"source":["###Define Model Architecture + Optimiser + Loss Functions\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AA9gv9zRpB7","executionInfo":{"status":"ok","timestamp":1608478582485,"user_tz":-330,"elapsed":11756,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"7df04ac6-299b-46f8-dfc1-c5928789f9da"},"source":["# Build model\n","#####################\n","input_dim = 1\n","hidden_dim = 32\n","num_layers = 2 \n","output_dim = 1\n","\n","\n","# Here we define our model as a class\n","class LSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTM, self).__init__()\n","        # Hidden dimensions\n","        self.hidden_dim = hidden_dim\n","\n","        # Number of hidden layers\n","        self.num_layers = num_layers\n","\n","        # Building your LSTM\n","        # batch_first=True causes input/output tensors to be of shape\n","        # (batch_dim, seq_dim, feature_dim)\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","\n","        # Readout layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Initialize hidden state with zeros\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().cuda()\n","\n","        # Initialize cell state\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().cuda()\n","\n","        # One time step\n","        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n","        # If we don't, we'll backprop all the way to the start even after going through another batch\n","        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n","\n","        # Index hidden state of last time step\n","        # out.size() --> 100, 28, 100\n","        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n","        out = self.fc(out[:, -1, :])\n","        # out.size() --> 100, 10\n","        return out\n","    \n","'''Location to save best performing model based on Validation Set Performance'''\n","\n","model_dir = \"/content/drive/My Drive/LSTM/Uni\"\n","\n","model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).cuda()\n","\n","loss_fn = torch.nn.MSELoss(size_average=True)\n","\n","optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n","print(model)\n","print(len(list(model.parameters())))\n","for i in range(len(list(model.parameters()))):\n","    print(list(model.parameters())[i].size())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LSTM(\n","  (lstm): LSTM(1, 32, num_layers=2, batch_first=True)\n","  (fc): Linear(in_features=32, out_features=1, bias=True)\n",")\n","10\n","torch.Size([128, 1])\n","torch.Size([128, 32])\n","torch.Size([128])\n","torch.Size([128])\n","torch.Size([128, 32])\n","torch.Size([128, 32])\n","torch.Size([128])\n","torch.Size([128])\n","torch.Size([1, 32])\n","torch.Size([1])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"rIx3_V0ZT7JX"},"source":["###Train Univariate LSTM Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRwAyJcpRtTU","executionInfo":{"status":"ok","timestamp":1608475097988,"user_tz":-330,"elapsed":607750,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"827f7639-c6c8-42af-db41-da57d83fcf79"},"source":["# Train model\n","#####################\n","\n","'''Set number of epochs. Default  50'''\n","num_epochs = 50\n","train_hist = np.zeros(num_epochs)\n","test_hist = np.zeros(num_epochs)\n","\n","# Number of steps to unroll\n","seq_dim = lookback-1  \n","\n","best_train_loss = 1e10\n","best_test_loss = 1e10\n","best_train_epoch = 0\n","best_test_epoch = 0\n","\n","\n","for t in range(num_epochs):\n","\n","      train_loss_epoch = 0\n","      test_loss_epoch = 0\n","\n","      model.train()\n","\n","      for x, y in train_loader:\n","\n","          x, y = x.cuda(), y.cuda()\n","\n","          y_pred  = model(x)\n","\n","          loss = loss_fn(y_pred, y)\n","\n","          train_loss_epoch += loss.item()\n","\n","          optimiser.zero_grad()\n","\n","          loss.backward()\n","          \n","          optimiser.step()\n","\n","          # if t % 10 == 0 and t !=0:\n","      print(\"Train Epoch \", t, \" Train MSE: \", train_loss_epoch)\n","      train_hist[t] = train_loss_epoch\n","\n","      if (train_loss_epoch < best_train_loss):\n","          best_train_loss = train_loss_epoch\n","          best_train_epoch = t\n","\n","      with torch.no_grad():\n","        \n","          for x, y in test_loader:\n","\n","              x, y = x.cuda(), y.cuda()\n","\n","              output = model(x) \n","\n","              loss = loss_fn(output, y)\n","\n","              test_loss_epoch += loss.item()\n","\n","      print(\"Validation Epoch \", t, \"Validation MSE: \", test_loss_epoch)\n","      test_hist[t] = test_loss_epoch\n","\n","      if (test_loss_epoch < best_test_loss):\n","          best_test_loss = test_loss_epoch\n","          best_test_epoch = t\n","\n","          torch.save({\n","                      'multi_lstm': model.state_dict(),\n","                      'optimizer_state_dict': optimiser.state_dict(),\n","                      }, os.path.join(model_dir, 'Mono_LSTM-{}-{}-{}-{}.pth'.format(to_do, lookback, best_test_epoch, best_test_loss)))\n","          \n","      print(train_loss_epoch, best_train_loss, test_loss_epoch, best_test_loss)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Epoch  0  Train MSE:  6.488508228108003\n","Validation Epoch  0 Validation MSE:  0.8921142063627485\n","6.488508228108003 6.488508228108003 0.8921142063627485 0.8921142063627485\n","Train Epoch  1  Train MSE:  2.7026635966905133\n","Validation Epoch  1 Validation MSE:  0.9002020554398769\n","2.7026635966905133 2.7026635966905133 0.9002020554398769 0.8921142063627485\n","Train Epoch  2  Train MSE:  2.245682020430422\n","Validation Epoch  2 Validation MSE:  0.6956993043568218\n","2.245682020430422 2.245682020430422 0.6956993043568218 0.6956993043568218\n","Train Epoch  3  Train MSE:  2.318307038556668\n","Validation Epoch  3 Validation MSE:  0.5522460303800472\n","2.318307038556668 2.245682020430422 0.5522460303800472 0.5522460303800472\n","Train Epoch  4  Train MSE:  1.8749866477119213\n","Validation Epoch  4 Validation MSE:  0.445939100747637\n","1.8749866477119213 1.8749866477119213 0.445939100747637 0.445939100747637\n","Train Epoch  5  Train MSE:  2.1189122545329155\n","Validation Epoch  5 Validation MSE:  0.539640090790499\n","2.1189122545329155 1.8749866477119213 0.539640090790499 0.445939100747637\n","Train Epoch  6  Train MSE:  2.236158408188203\n","Validation Epoch  6 Validation MSE:  0.5499335456988774\n","2.236158408188203 1.8749866477119213 0.5499335456988774 0.445939100747637\n","Train Epoch  7  Train MSE:  1.745263671605585\n","Validation Epoch  7 Validation MSE:  0.42733209463176536\n","1.745263671605585 1.745263671605585 0.42733209463176536 0.42733209463176536\n","Train Epoch  8  Train MSE:  1.65837880248705\n","Validation Epoch  8 Validation MSE:  0.42876352744860924\n","1.65837880248705 1.65837880248705 0.42876352744860924 0.42733209463176536\n","Train Epoch  9  Train MSE:  1.983102097141341\n","Validation Epoch  9 Validation MSE:  0.433179114577797\n","1.983102097141341 1.65837880248705 0.433179114577797 0.42733209463176536\n","Train Epoch  10  Train MSE:  2.352268181830368\n","Validation Epoch  10 Validation MSE:  0.4486299593659169\n","2.352268181830368 1.65837880248705 0.4486299593659169 0.42733209463176536\n","Train Epoch  11  Train MSE:  1.754757857536788\n","Validation Epoch  11 Validation MSE:  0.42705315423518186\n","1.754757857536788 1.65837880248705 0.42705315423518186 0.42705315423518186\n","Train Epoch  12  Train MSE:  1.6690126152610105\n","Validation Epoch  12 Validation MSE:  0.42296869978940776\n","1.6690126152610105 1.65837880248705 0.42296869978940776 0.42296869978940776\n","Train Epoch  13  Train MSE:  1.6600917059331266\n","Validation Epoch  13 Validation MSE:  0.4203008662972536\n","1.6600917059331266 1.65837880248705 0.4203008662972536 0.4203008662972536\n","Train Epoch  14  Train MSE:  1.7341547745862727\n","Validation Epoch  14 Validation MSE:  0.4158517854061756\n","1.7341547745862727 1.65837880248705 0.4158517854061756 0.4158517854061756\n","Train Epoch  15  Train MSE:  1.6580639846209237\n","Validation Epoch  15 Validation MSE:  0.4121938578937261\n","1.6580639846209237 1.6580639846209237 0.4121938578937261 0.4121938578937261\n","Train Epoch  16  Train MSE:  1.5847639500304922\n","Validation Epoch  16 Validation MSE:  0.417260498997166\n","1.5847639500304922 1.5847639500304922 0.417260498997166 0.4121938578937261\n","Train Epoch  17  Train MSE:  1.5929864966356035\n","Validation Epoch  17 Validation MSE:  0.4219075227210851\n","1.5929864966356035 1.5847639500304922 0.4219075227210851 0.4121938578937261\n","Train Epoch  18  Train MSE:  1.6888044790857748\n","Validation Epoch  18 Validation MSE:  0.407222449889332\n","1.6888044790857748 1.5847639500304922 0.407222449889332 0.407222449889332\n","Train Epoch  19  Train MSE:  1.5401386307789835\n","Validation Epoch  19 Validation MSE:  0.39775622854853054\n","1.5401386307789835 1.5401386307789835 0.39775622854853054 0.39775622854853054\n","Train Epoch  20  Train MSE:  1.6585652729841058\n","Validation Epoch  20 Validation MSE:  0.4040842834186549\n","1.6585652729841058 1.5401386307789835 0.4040842834186549 0.39775622854853054\n","Train Epoch  21  Train MSE:  1.8104971055292083\n","Validation Epoch  21 Validation MSE:  0.40320803755651013\n","1.8104971055292083 1.5401386307789835 0.40320803755651013 0.39775622854853054\n","Train Epoch  22  Train MSE:  1.7358269180086836\n","Validation Epoch  22 Validation MSE:  0.40427787384794556\n","1.7358269180086836 1.5401386307789835 0.40427787384794556 0.39775622854853054\n","Train Epoch  23  Train MSE:  1.568354762483068\n","Validation Epoch  23 Validation MSE:  0.39819267205632514\n","1.568354762483068 1.5401386307789835 0.39819267205632514 0.39775622854853054\n","Train Epoch  24  Train MSE:  1.582436718479471\n","Validation Epoch  24 Validation MSE:  0.40207327854386676\n","1.582436718479471 1.5401386307789835 0.40207327854386676 0.39775622854853054\n","Train Epoch  25  Train MSE:  1.5708166879712735\n","Validation Epoch  25 Validation MSE:  0.3977184900734301\n","1.5708166879712735 1.5401386307789835 0.3977184900734301 0.3977184900734301\n","Train Epoch  26  Train MSE:  1.572333755326099\n","Validation Epoch  26 Validation MSE:  0.3977286115736547\n","1.572333755326099 1.5401386307789835 0.3977286115736547 0.3977184900734301\n","Train Epoch  27  Train MSE:  1.5605438980496729\n","Validation Epoch  27 Validation MSE:  0.39504926121605877\n","1.5605438980496729 1.5401386307789835 0.39504926121605877 0.39504926121605877\n","Train Epoch  28  Train MSE:  1.5375810373770946\n","Validation Epoch  28 Validation MSE:  0.3912756627998135\n","1.5375810373770946 1.5375810373770946 0.3912756627998135 0.3912756627998135\n","Train Epoch  29  Train MSE:  1.5449851748285255\n","Validation Epoch  29 Validation MSE:  0.39005362617297124\n","1.5449851748285255 1.5375810373770946 0.39005362617297124 0.39005362617297124\n","Train Epoch  30  Train MSE:  1.5690674455649969\n","Validation Epoch  30 Validation MSE:  0.3915067847474347\n","1.5690674455649969 1.5375810373770946 0.3915067847474347 0.39005362617297124\n","Train Epoch  31  Train MSE:  1.5336189203169397\n","Validation Epoch  31 Validation MSE:  0.3920574134488106\n","1.5336189203169397 1.5336189203169397 0.3920574134488106 0.39005362617297124\n","Train Epoch  32  Train MSE:  1.5127558105691605\n","Validation Epoch  32 Validation MSE:  0.38886284981163044\n","1.5127558105691605 1.5127558105691605 0.38886284981163044 0.38886284981163044\n","Train Epoch  33  Train MSE:  1.5313543653076067\n","Validation Epoch  33 Validation MSE:  0.388313394695615\n","1.5313543653076067 1.5127558105691605 0.388313394695615 0.388313394695615\n","Train Epoch  34  Train MSE:  1.5491560205766746\n","Validation Epoch  34 Validation MSE:  0.3917540314992607\n","1.5491560205766746 1.5127558105691605 0.3917540314992607 0.388313394695615\n","Train Epoch  35  Train MSE:  1.665642256280762\n","Validation Epoch  35 Validation MSE:  0.3962824117403443\n","1.665642256280762 1.5127558105691605 0.3962824117403443 0.388313394695615\n","Train Epoch  36  Train MSE:  1.497748113720263\n","Validation Epoch  36 Validation MSE:  0.38648250243159055\n","1.497748113720263 1.497748113720263 0.38648250243159055 0.38648250243159055\n","Train Epoch  37  Train MSE:  1.5464109028932853\n","Validation Epoch  37 Validation MSE:  0.387954965606923\n","1.5464109028932853 1.497748113720263 0.387954965606923 0.38648250243159055\n","Train Epoch  38  Train MSE:  1.5457924174135087\n","Validation Epoch  38 Validation MSE:  0.3874403550989882\n","1.5457924174135087 1.497748113720263 0.3874403550989882 0.38648250243159055\n","Train Epoch  39  Train MSE:  1.4910433741779343\n","Validation Epoch  39 Validation MSE:  0.3855807872366199\n","1.4910433741779343 1.4910433741779343 0.3855807872366199 0.3855807872366199\n","Train Epoch  40  Train MSE:  1.498822545618168\n","Validation Epoch  40 Validation MSE:  0.3856009041675179\n","1.498822545618168 1.4910433741779343 0.3856009041675179 0.3855807872366199\n","Train Epoch  41  Train MSE:  1.6035843639210725\n","Validation Epoch  41 Validation MSE:  0.38766919722979765\n","1.6035843639210725 1.4910433741779343 0.38766919722979765 0.3855807872366199\n","Train Epoch  42  Train MSE:  1.588663894299998\n","Validation Epoch  42 Validation MSE:  0.3910603793269729\n","1.588663894299998 1.4910433741779343 0.3910603793269729 0.3855807872366199\n","Train Epoch  43  Train MSE:  1.4884441417653989\n","Validation Epoch  43 Validation MSE:  0.3836056423867831\n","1.4884441417653989 1.4884441417653989 0.3836056423867831 0.3836056423867831\n","Train Epoch  44  Train MSE:  1.4638950743442365\n","Validation Epoch  44 Validation MSE:  0.3832133790061789\n","1.4638950743442365 1.4638950743442365 0.3832133790061789 0.3832133790061789\n","Train Epoch  45  Train MSE:  1.4924819464899883\n","Validation Epoch  45 Validation MSE:  0.38377591337777517\n","1.4924819464899883 1.4638950743442365 0.38377591337777517 0.3832133790061789\n","Train Epoch  46  Train MSE:  1.4815340037830538\n","Validation Epoch  46 Validation MSE:  0.38486959849814184\n","1.4815340037830538 1.4638950743442365 0.38486959849814184 0.3832133790061789\n","Train Epoch  47  Train MSE:  1.4868021962240618\n","Validation Epoch  47 Validation MSE:  0.38381972320837576\n","1.4868021962240618 1.4638950743442365 0.38381972320837576 0.3832133790061789\n","Train Epoch  48  Train MSE:  1.484385149930631\n","Validation Epoch  48 Validation MSE:  0.3847509075585549\n","1.484385149930631 1.4638950743442365 0.3847509075585549 0.3832133790061789\n","Train Epoch  49  Train MSE:  1.4803371156117464\n","Validation Epoch  49 Validation MSE:  0.3857730797790282\n","1.4803371156117464 1.4638950743442365 0.3857730797790282 0.3832133790061789\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BYo9sI3DUheN"},"source":["##Predicting the regression values and state Labels\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mHk7Cwd5XuNf"},"source":["###Load Best Performing Univariate Models for each of the columns"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pq5r-43yxyMj","executionInfo":{"status":"ok","timestamp":1608478585060,"user_tz":-330,"elapsed":9510,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"9625d923-b914-46c7-fb85-cdde5b32a72c"},"source":["model_dir = \"/content/drive/My Drive/LSTM\"\n","\n","uni_nfr_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).cuda()\n","uni_mono_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).cuda()\n","uni_di_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).cuda()\n","uni_tri_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).cuda()\n","\n","\n","checkpointA = torch.load(os.path.join(model_dir, 'Mono_LSTM-nfr-20-40-7.361742030829191.pth'))\n","checkpointB = torch.load(os.path.join(model_dir, 'Mono_LSTM-mono-20-38-10.677848100662231.pth'))\n","checkpointC = torch.load(os.path.join(model_dir, 'Mono_LSTM-di-20-40-3.1974508333951235.pth'))\n","checkpointD = torch.load(os.path.join(model_dir, 'Mono_LSTM-tri-20-44-0.3832133790061789.pth'))\n","\n","uni_nfr_model.load_state_dict(checkpointA['multi_lstm'])\n","uni_mono_model.load_state_dict(checkpointB['multi_lstm'])\n","uni_di_model.load_state_dict(checkpointC['multi_lstm'])\n","uni_tri_model.load_state_dict(checkpointD['multi_lstm'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"vlP7AYnUUrcz"},"source":["###Load CSV file on which to Predict"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"bq8qAzG33m_G","executionInfo":{"status":"ok","timestamp":1608479946879,"user_tz":-330,"elapsed":1029,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"638cba5b-8641-40b1-ba83-f75d74e953ac"},"source":["import numpy as np\n","import pandas as pd\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","path = \"/content/drive/My Drive/project/\"\n","training_dataset = pd.read_csv(path + 'split-29.csv')\n","\n","training_dataset.columns = ['nfr','mono','di','tri']\n","training_dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>nfr</th>\n","      <th>mono</th>\n","      <th>di</th>\n","      <th>tri</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6.5921</td>\n","      <td>1.1372</td>\n","      <td>0.0001</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6.5921</td>\n","      <td>1.1372</td>\n","      <td>0.0001</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6.5921</td>\n","      <td>1.1372</td>\n","      <td>0.0001</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      nfr    mono      di  tri\n","0  0.0000  0.0000  0.0000  0.0\n","1  0.0000  0.0000  0.0000  0.0\n","2  6.5921  1.1372  0.0001  0.0\n","3  6.5921  1.1372  0.0001  0.0\n","4  6.5921  1.1372  0.0001  0.0"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"NS4SdbTwUv5j"},"source":["###Predict values for each of the columns\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnpTsY2y7cGp","executionInfo":{"status":"ok","timestamp":1608480200115,"user_tz":-330,"elapsed":45330,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"817a203c-5548-438a-bacd-cebf8bb22a22"},"source":["final_data = []\n","\n","model_list = [uni_nfr_model, uni_mono_model, uni_di_model, uni_tri_model]\n","col_list = ['nfr', 'mono', 'di', 'tri']\n","\n","for i in range(len(col_list)):\n","  to_do = col_list[i]\n","  model = model_list[i]\n","  price = training_dataset[[to_do]]\n","\n","  def pred_data(stock, lookback):\n","      data_raw = stock.to_numpy() # convert to numpy array\n","      data = []\n","      \n","      # create all possible sequences of length seq_len\n","      for index in range(len(data_raw) - lookback): \n","          data.append(data_raw[index: index + lookback])\n","      \n","      data = np.array(data)\n","      train_set_size = data.shape[0]\n","      \n","      x_train = data[:train_set_size,:-1,:]\n","      y_train = data[:train_set_size,-1,:]\n","      \n","      return [x_train, y_train]\n","\n","  x_pred, y_pred = pred_data(price, lookback)\n","\n","  x_pred = torch.from_numpy(x_pred).type(torch.Tensor)\n","  y_pred = torch.from_numpy(y_pred).type(torch.Tensor)\n","\n","  pred = torch.utils.data.TensorDataset(x_pred,y_pred)\n","\n","  pred_loader = torch.utils.data.DataLoader(dataset=pred, \n","                                            batch_size=batch_size, \n","                                            shuffle=False)\n","\n","  # print(training_dataset.shape)\n","  # print(x_pred.shape, y_pred.shape)\n","\n","  predictions = []\n","  with torch.no_grad():\n","\n","    for x, y in pred_loader:\n","\n","            x, y = x.cuda(), y.cuda()\n","\n","            output = uni_nfr_model(x).view(-1).cpu().numpy()\n","\n","            predictions.extend(output)\n","              \n","  temp = price.iloc[0:lookback].values.flatten()\n","  predictions = np.append(temp, predictions)\n","\n","  final_data.append(predictions)\n","  print(final_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(999999, 4)\n","torch.Size([999979, 19, 1]) torch.Size([999979, 1])\n","[array([0.        , 0.        , 6.5921    , ..., 5.90757418, 5.99875689,\n","       5.98339081])]\n","(999999, 4)\n","torch.Size([999979, 19, 1]) torch.Size([999979, 1])\n","[array([0.        , 0.        , 6.5921    , ..., 5.90757418, 5.99875689,\n","       5.98339081]), array([0.        , 0.        , 1.1372    , ..., 0.63269067, 0.62940854,\n","       0.63487089])]\n","(999999, 4)\n","torch.Size([999979, 19, 1]) torch.Size([999979, 1])\n","[array([0.        , 0.        , 6.5921    , ..., 5.90757418, 5.99875689,\n","       5.98339081]), array([0.        , 0.        , 1.1372    , ..., 0.63269067, 0.62940854,\n","       0.63487089]), array([0.        , 0.        , 0.0001    , ..., 0.02607608, 0.02607608,\n","       0.02607608])]\n","(999999, 4)\n","torch.Size([999979, 19, 1]) torch.Size([999979, 1])\n","[array([0.        , 0.        , 6.5921    , ..., 5.90757418, 5.99875689,\n","       5.98339081]), array([0.        , 0.        , 1.1372    , ..., 0.63269067, 0.62940854,\n","       0.63487089]), array([0.        , 0.        , 0.0001    , ..., 0.02607608, 0.02607608,\n","       0.02607608]), array([0.        , 0.        , 0.        , ..., 0.02607608, 0.02607608,\n","       0.02607608])]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fZS-Gt136ZgX","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1608480228498,"user_tz":-330,"elapsed":3993,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"34a22b22-434c-4137-a1ba-c73041147ad4"},"source":["temp = []\n","\n","for i in range(len(final_data[0])):\n","  temp.append([final_data[0][i],final_data[1][i],final_data[2][i],final_data[3][i]])\n","\n","df = pd.DataFrame(temp, columns=col_list)\n","\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>nfr</th>\n","      <th>mono</th>\n","      <th>di</th>\n","      <th>tri</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6.592100</td>\n","      <td>1.137200</td>\n","      <td>0.000100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6.592100</td>\n","      <td>1.137200</td>\n","      <td>0.000100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6.592100</td>\n","      <td>1.137200</td>\n","      <td>0.000100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>999994</th>\n","      <td>5.850145</td>\n","      <td>0.625675</td>\n","      <td>0.026076</td>\n","      <td>0.026076</td>\n","    </tr>\n","    <tr>\n","      <th>999995</th>\n","      <td>5.811829</td>\n","      <td>0.621055</td>\n","      <td>0.026076</td>\n","      <td>0.026076</td>\n","    </tr>\n","    <tr>\n","      <th>999996</th>\n","      <td>5.907574</td>\n","      <td>0.632691</td>\n","      <td>0.026076</td>\n","      <td>0.026076</td>\n","    </tr>\n","    <tr>\n","      <th>999997</th>\n","      <td>5.998757</td>\n","      <td>0.629409</td>\n","      <td>0.026076</td>\n","      <td>0.026076</td>\n","    </tr>\n","    <tr>\n","      <th>999998</th>\n","      <td>5.983391</td>\n","      <td>0.634871</td>\n","      <td>0.026076</td>\n","      <td>0.026076</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>999999 rows × 4 columns</p>\n","</div>"],"text/plain":["             nfr      mono        di       tri\n","0       0.000000  0.000000  0.000000  0.000000\n","1       0.000000  0.000000  0.000000  0.000000\n","2       6.592100  1.137200  0.000100  0.000000\n","3       6.592100  1.137200  0.000100  0.000000\n","4       6.592100  1.137200  0.000100  0.000000\n","...          ...       ...       ...       ...\n","999994  5.850145  0.625675  0.026076  0.026076\n","999995  5.811829  0.621055  0.026076  0.026076\n","999996  5.907574  0.632691  0.026076  0.026076\n","999997  5.998757  0.629409  0.026076  0.026076\n","999998  5.983391  0.634871  0.026076  0.026076\n","\n","[999999 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"y1MC8QfeU0yl"},"source":["###Run K-Means to assign State Labels for each row"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"iCxGSTb66C1o","executionInfo":{"status":"ok","timestamp":1608487074450,"user_tz":-330,"elapsed":8781,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"7ea1d3ad-d387-4b9a-8076-00990b32bc11"},"source":["n_samples, n_features = df.shape\n","n_classes = 3\n","estimator = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)\n","estimator.fit(df)\n","labels = estimator.predict(df)\n","labels = pd.DataFrame(labels)\n","estimator.cluster_centers_\n","nfr_values = np.array([estimator.cluster_centers_[i][0] for i in range(3)])\n","index_C = np.argmax(nfr_values)\n","index_B = np.argmin(nfr_values)\n","index_L = 3 - index_C - index_B\n","\n","labels = labels.replace({index_C:'C', index_B:'B', index_L:'L'})\n","labels.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[497421. 832043. 165377.]\n","0\n","2\n","1\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   0\n","0  B\n","1  B\n","2  B\n","3  B\n","4  B"]},"metadata":{"tags":[]},"execution_count":126}]},{"cell_type":"markdown","metadata":{"id":"DlyFa_jMU4R_"},"source":["###Save the Regression and Label Prediction Files to the drive"]},{"cell_type":"code","metadata":{"id":"HkY1uzQMNVm9"},"source":["df.to_csv('mono_pred.csv')\n","!cp mono_pred.csv \"drive/My Drive/\"\n","\n","labels.to_csv('mono_labels.csv')\n","!cp mono_labels.csv \"drive/My Drive/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbKqdRiPzAMf"},"source":["#Multivariate LSTM"]},{"cell_type":"markdown","metadata":{"id":"eHUiqZRbVxlI"},"source":["##Training LSTM Models\n","\n","###Define Model Architecture"]},{"cell_type":"code","metadata":{"id":"4mAFpWSIMeWI"},"source":["class MV_LSTM(torch.nn.Module):\n","    def __init__(self,n_features,seq_length):\n","        super(MV_LSTM, self).__init__()\n","        self.n_features = n_features\n","        self.seq_len = seq_length\n","        self.n_hidden = 20 # number of hidden states\n","        self.n_layers = 1 # number of LSTM layers (stacked)\n","    \n","        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n","                                 hidden_size = self.n_hidden,\n","                                 num_layers = self.n_layers, \n","                                 batch_first = True)\n","        # according to pytorch docs LSTM output is \n","        # (batch_size,seq_len, num_directions * hidden_size)\n","        # when considering batch_first = True\n","        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n","        \n","    \n","    def init_hidden(self, batch_size):\n","        # even with batch_first = True this remains same as docs\n","        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden).requires_grad_().cuda()\n","        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden).requires_grad_().cuda()\n","        self.hidden = (hidden_state, cell_state)\n","    \n","    \n","    def forward(self, x):        \n","        batch_size, seq_len, _ = x.size()\n","        \n","        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n","        # lstm_out(with batch_first = True) is \n","        # (batch_size,seq_len,num_directions * hidden_size)\n","        # for following linear layer we want to keep batch_size dimension and merge rest       \n","        # .contiguous() -> solves tensor compatibility error\n","        x = lstm_out.contiguous().view(batch_size,-1)\n","        return self.l_linear(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYWNTejrNKxS","executionInfo":{"status":"ok","timestamp":1608482739211,"user_tz":-330,"elapsed":5341,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"2871fb4a-0c43-4d61-c02b-ccb7afd94fae"},"source":["dataset = training_dataset\n","print(dataset.head)\n","print(dataset.iloc[0:3, -1])\n","print(dataset[0:3][\"tri\"])\n","\n","dataset = training_dataset.to_numpy()\n","print(dataset)\n","# print(dataset.iloc[[0:3],[1,2]])\n","# dataset.iloc[1:3]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<bound method NDFrame.head of               nfr      mono        di       tri\n","0        2.148752  0.190306  2.152600  0.088854\n","1        2.148752  0.190306  2.152600  0.088854\n","2        2.148752  0.190306  2.152600  0.088854\n","3        2.148752  0.190306  2.152600  0.088854\n","4        1.222059  1.638257  2.155221  0.088854\n","...           ...       ...       ...       ...\n","1001892  0.000000  0.000000  0.000000  0.000000\n","1001893  0.000000  0.000000  0.000000  0.000000\n","1001894  0.000000  0.000000  0.000000  0.000000\n","1001895  0.000000  0.000000  0.000000  0.000000\n","1001896  0.000000  0.000000  0.000000  0.000000\n","\n","[1001897 rows x 4 columns]>\n","0    0.088854\n","1    0.088854\n","2    0.088854\n","Name: tri, dtype: float64\n","0    0.088854\n","1    0.088854\n","2    0.088854\n","Name: tri, dtype: float64\n","[[2.14875224 0.1903058  2.15259968 0.08885376]\n"," [2.14875224 0.1903058  2.15259968 0.08885376]\n"," [2.14875224 0.1903058  2.15259968 0.08885376]\n"," ...\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-foOhQ6CV9AH"},"source":["###Process the dataset to adjust for context window."]},{"cell_type":"code","metadata":{"id":"O-PHeo-jNWS4"},"source":["# split a multivariate sequence into samples\n","def split_sequences(sequences, n_steps, n_features, to_do):\n","    X, y = list(), list()\n","    for i in range(len(sequences)):\n","        # find the end of this pattern\n","        end_ix = i + n_steps\n","        # check if we are beyond the dataset\n","        if end_ix + 1 > len(sequences):\n","            break\n","        # gather input and output parts of the pattern\n","        seq_x, seq_y = sequences[i:end_ix, :n_features], sequences[end_ix, -mapping[to_do]]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","\n","        # print(X)\n","        # print(y)\n","\n","        # break\n","    return np.array(X), np.array(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"krVaiEVkMot1","executionInfo":{"status":"ok","timestamp":1608482745887,"user_tz":-330,"elapsed":2544,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"3f9f7efe-46f4-4359-e3dd-7002782d3e03"},"source":["from array import array\n","\n","''' The user needs to train the LSTM for each of the columns in the dataset.\n","    This is made possible by setting the to_do value to the particular column name.\n","    For example, here to_do = 'nfr' trains an LSTM over the nfr column values.\n","    If you want to train an LSTM for each of the columns, simply change the value of to_do (uncomment the other line and comment the current one)\n","'''\n","\n","to_do = 'nfr'\n","# to_do = 'mono'\n","# to_do = 'di'\n","# to_do = 'tri'\n","\n","mapping = {'nfr':0, 'mono':1, 'di':2, 'tri':3}\n","\n","n_features = 4 # this is number of parallel inputs\n","n_timesteps = 20 # this is number of timesteps\n","look_back = 20\n","\n","# convert dataset into input/output\n","X, y = split_sequences(dataset, n_timesteps, n_features, to_do)\n","print(X.shape, y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1001877, 20, 4) (1001877,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hExoG9EPWGCT"},"source":["###Split into Train and Validation Set (Default 80-20 ratio) and Create DataLoaders"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lvKei55x5ev","executionInfo":{"status":"ok","timestamp":1608482745891,"user_tz":-330,"elapsed":981,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"2a5787b9-9de0-417f-d228-f8d4c2da1198"},"source":["ratio = 0.2\n","\n","test_set_size = int(np.round(ratio*X.shape[0]));\n","train_set_size = X.shape[0] - (test_set_size);\n","\n","x_train = X[:train_set_size]\n","y_train = y[:train_set_size]\n","\n","x_test = X[train_set_size:]\n","y_test = y[train_set_size:]\n","\n","x_train = torch.from_numpy(x_train).type(torch.Tensor)\n","x_test = torch.from_numpy(x_test).type(torch.Tensor)\n","y_train = torch.from_numpy(y_train).type(torch.Tensor)\n","y_test = torch.from_numpy(y_test).type(torch.Tensor)\n","\n","print(x_train.shape)\n","print(y_train.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([801502, 20, 4])\n","torch.Size([801502])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2KCgZR5UWQJh"},"source":["###Define Optimiser + Loss Function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"458F0_1QXw2g"},"source":["###Train Multivariate LSTM Models"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"XXgWxdknM7Ri","executionInfo":{"status":"error","timestamp":1608482749883,"user_tz":-330,"elapsed":4695,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"dc2e7579-2561-4b2b-b287-c9fdf89600b1"},"source":["# create NN\n","mv_net = MV_LSTM(n_features,n_timesteps).cuda()\n","criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n","optimizer = torch.optim.Adam(mv_net.parameters(), lr=0.001)\n","\n","train_episodes = 20\n","num_epochs = train_episodes\n","batch_size = 32\n","mv_net.train()\n","\n","\n","train = torch.utils.data.TensorDataset(x_train,y_train)\n","test = torch.utils.data.TensorDataset(x_test,y_test)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train, \n","                                           batch_size=batch_size, \n","                                           shuffle=False)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)\n","\n","\n","train_hist = np.zeros(num_epochs)\n","test_hist = np.zeros(num_epochs)\n","\n","best_train_loss = 1e10\n","best_test_loss = 1e10\n","best_train_epoch = 0\n","best_test_epoch = 0\n","\n","\n","for t in range(train_episodes):\n","\n","    train_loss_epoch = 0\n","    test_loss_epoch = 0\n","\n","    for x, y in train_loader:\n","        x_batch = x.cuda()\n","        y_batch = y.cuda()\n","    \n","        mv_net.init_hidden(x_batch.size(0))\n","\n","        output = mv_net(x_batch) \n","        loss = criterion(output.view(-1), y_batch)  \n","\n","        train_loss_epoch += loss.item()\n","        \n","        loss.backward()\n","        optimizer.step()        \n","        optimizer.zero_grad() \n","\n","    print(\"Train Epoch \", t, \" Train MSE: \", train_loss_epoch)\n","    train_hist[t] = train_loss_epoch\n","\n","    if (train_loss_epoch < best_train_loss):\n","        best_train_loss = train_loss_epoch\n","        best_train_epoch = t\n","\n","    with torch.no_grad():\n","\n","        for x, y in test_loader:\n","\n","            x_batch = x.cuda()\n","            y_batch = y.cuda()\n","        \n","            mv_net.init_hidden(x_batch.size(0))\n","\n","            output = mv_net(x_batch) \n","\n","            loss = criterion(output.view(-1), y_batch) \n","\n","            test_loss_epoch += loss.item()\n","\n","        print(\"Validation Epoch \", t, \"Validation MSE: \", test_loss_epoch)\n","        test_hist[t] = test_loss_epoch\n","\n","        if (test_loss_epoch < best_test_loss):\n","            best_test_loss = test_loss_epoch\n","            best_test_epoch = t\n","\n","            torch.save({\n","                        'multi_lstm': mv_net.state_dict(),\n","                        'optimizer_state_dict': optimizer.state_dict(),\n","                        }, os.path.join(model_dir, 'Multi_LSTM-{}-{}-{}-{}.pth'.format(to_do, lookback, best_test_epoch, best_test_loss)))\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-0e5a663d27c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtrain_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"7UelZjR2WlVn"},"source":["##Predicting the regression values and State Labels"]},{"cell_type":"markdown","metadata":{"id":"Enu9_4nSX3uM"},"source":["###Load Best Performing Multivariate Models for each of the columns"]},{"cell_type":"code","metadata":{"id":"l3TZiOcv4yVc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608484464720,"user_tz":-330,"elapsed":1241,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"1bb7af43-e563-48d6-88c0-4e5736574a26"},"source":["import os\n","model_dir = \"/content/drive/My Drive/LSTM\"\n","\n","# !ls $model_dir\n","multi_nfr_model = MV_LSTM(n_features,n_timesteps).cuda()\n","multi_mono_model = MV_LSTM(n_features,n_timesteps).cuda()\n","multi_di_model = MV_LSTM(n_features,n_timesteps).cuda()\n","multi_tri_model = MV_LSTM(n_features,n_timesteps).cuda()\n","\n","\n","checkpointA = torch.load(os.path.join(model_dir, 'Multi_LSTM-nfr-20-0.pth'))\n","checkpointB = torch.load(os.path.join(model_dir, 'Multi_LSTM-mono-20-0.pth'))\n","checkpointC = torch.load(os.path.join(model_dir, 'Multi_LSTM-di-20-0.pth'))\n","checkpointD = torch.load(os.path.join(model_dir, 'Multi_LSTM-tri-20-0.pth'))\n","\n","multi_nfr_model.load_state_dict(checkpointA['multi_lstm'])\n","multi_mono_model.load_state_dict(checkpointB['multi_lstm'])\n","multi_di_model.load_state_dict(checkpointC['multi_lstm'])\n","multi_tri_model.load_state_dict(checkpointD['multi_lstm'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"0hJRCH8fJlHX"},"source":["# split a multivariate sequence into samples\n","def split_sequences(sequences, n_steps, n_features, to_do):\n","    X, y = list(), list()\n","    for i in range(len(sequences)):\n","        # find the end of this pattern\n","        end_ix = i + n_steps\n","        # check if we are beyond the dataset\n","        if end_ix + 1 > len(sequences):\n","            break\n","        # gather input and output parts of the pattern\n","        seq_x, seq_y = sequences[i:end_ix, :n_features], sequences[end_ix, -mapping[to_do]]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","\n","        # print(X)\n","        # print(y)\n","\n","        # break\n","    return np.array(X), np.array(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0RfTYKndYCDQ"},"source":["###Load CSV file on which to predict\n"]},{"cell_type":"markdown","metadata":{"id":"6dPo9UPcYSHb"},"source":["###Predict values for each of the columns"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SzURcEYKFC9_","executionInfo":{"status":"ok","timestamp":1608484916040,"user_tz":-330,"elapsed":143456,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"497bc7c2-ce6f-4ed9-ee79-f77da66be3cf"},"source":["path = \"/content/drive/My Drive/project/\"\n","dataset = pd.read_csv(path + 'split-29.csv')\n","dataset = dataset.to_numpy()\n","\n","model_list = [multi_nfr_model, multi_mono_model, multi_di_model, multi_tri_model]\n","col_list = ['nfr', 'mono', 'di', 'tri']\n","final_data = []\n","\n","for i in range(len(col_list)):\n","  to_do = col_list[i]\n","  mvnet = model_list[i]\n","\n","  X_pred, y_pred = split_sequences(dataset, n_timesteps, n_features, to_do)\n","\n","  X_pred = torch.from_numpy(X_pred).type(torch.Tensor)\n","  y_pred = torch.from_numpy(y_pred).type(torch.Tensor)\n","\n","  pred = torch.utils.data.TensorDataset(X_pred,y_pred)\n","\n","  pred_loader = torch.utils.data.DataLoader(dataset=pred, \n","                                            batch_size=batch_size, \n","                                            shuffle=False)\n","\n","  print(X_pred.shape)\n","# predictions = price.iloc[0:lookback]\n","\n","  predictions = []\n","  total_loss = 0\n","\n","  with torch.no_grad():\n","\n","    for x, y in pred_loader:\n","\n","            x_batch, y_batch = x.cuda(), y.cuda()\n","\n","            mv_net.init_hidden(x_batch.size(0))\n","\n","            output = mv_net(x_batch) \n","            loss = criterion(output.view(-1), y_batch)  \n","\n","            predictions.extend(output.view(-1).cpu().numpy())\n","            # break\n","\n","            total_loss += loss.item()\n","\n","\n","  print(total_loss)\n","\n","\n","  temp = y_pred.flatten().numpy()[0:look_back]\n","  predictions = np.append(temp, predictions)\n","\n","  final_data.append(predictions)\n","  print(final_data)\n","  print(len(final_data[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([999979, 20, 4])\n","458374.8463915407\n","[array([6.5921   , 6.5921   , 6.5921   , ..., 1.3147007, 1.2994127,\n","       1.2938184], dtype=float32)]\n","999999\n","torch.Size([999979, 20, 4])\n","988384.0513844554\n","[array([6.5921   , 6.5921   , 6.5921   , ..., 1.3147007, 1.2994127,\n","       1.2938184], dtype=float32), array([0.       , 0.       , 0.       , ..., 1.3147007, 1.2994127,\n","       1.2938184], dtype=float32)]\n","999999\n","torch.Size([999979, 20, 4])\n","705339.2026189344\n","[array([6.5921   , 6.5921   , 6.5921   , ..., 1.3147007, 1.2994127,\n","       1.2938184], dtype=float32), array([0.       , 0.       , 0.       , ..., 1.3147007, 1.2994127,\n","       1.2938184], dtype=float32), array([9.9999997e-05, 9.9999997e-05, 9.9999997e-05, ..., 1.3147007e+00,\n","       1.2994127e+00, 1.2938184e+00], dtype=float32)]\n","999999\n","torch.Size([999979, 20, 4])\n","71768.68248007144\n","[array([6.5921   , 6.5921   , 6.5921   , ..., 1.3147007, 1.2994127,\n","       1.2938184], dtype=float32), array([0.       , 0.       , 0.       , ..., 1.3147007, 1.2994127,\n","       1.2938184], dtype=float32), array([9.9999997e-05, 9.9999997e-05, 9.9999997e-05, ..., 1.3147007e+00,\n","       1.2994127e+00, 1.2938184e+00], dtype=float32), array([1.1372   , 1.1372   , 1.1372   , ..., 1.3147007, 1.2994127,\n","       1.2938184], dtype=float32)]\n","999999\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKnnzpWGIszl","executionInfo":{"status":"ok","timestamp":1608484930079,"user_tz":-330,"elapsed":3042,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"54be7354-db88-43d2-9fe9-ae47bed22226"},"source":["temp = []\n","\n","for i in range(len(final_data[0])):\n","  temp.append([final_data[0][i],final_data[1][i],final_data[2][i],final_data[3][i]])\n","\n","df = pd.DataFrame(temp, columns=['nfr', 'mono', 'di', 'tri'])\n","print(df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["             nfr      mono        di       tri\n","0       6.592100  0.000000  0.000100  1.137200\n","1       6.592100  0.000000  0.000100  1.137200\n","2       6.592100  0.000000  0.000100  1.137200\n","3       6.592100  0.000000  0.000100  1.137200\n","4       6.592100  0.000000  0.000100  1.137200\n","...          ...       ...       ...       ...\n","999994  1.303105  1.303105  1.303105  1.303105\n","999995  1.299840  1.299840  1.299840  1.299840\n","999996  1.314701  1.314701  1.314701  1.314701\n","999997  1.299413  1.299413  1.299413  1.299413\n","999998  1.293818  1.293818  1.293818  1.293818\n","\n","[999999 rows x 4 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vhXN8m_IYXeI"},"source":["###Run K-Means to assign State Labels for each row"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"bvXLX4ZgKlut","executionInfo":{"status":"ok","timestamp":1608493454184,"user_tz":-330,"elapsed":7759,"user":{"displayName":"Computational Genomics","photoUrl":"","userId":"17431518683290654409"}},"outputId":"e258b859-938d-4434-a6b0-e4be7b9caf54"},"source":["n_samples, n_features = df.shape\n","n_classes = 3\n","estimator = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)\n","estimator.fit(df)\n","labels = estimator.predict(df)\n","labels = pd.DataFrame(labels)\n","print(estimator.cluster_centers_)\n","nfr_values = np.array([estimator.cluster_centers_[i][0] for i in range(3)])\n","index_C = np.argmax(nfr_values)\n","index_B = np.argmin(nfr_values)\n","index_L = 3 - index_C - index_B\n","print(nfr_values)\n","print(index_L)\n","print(index_B)\n","print(index_C)\n","labels = labels.replace({index_C:'C', index_B:'B', index_L:'L'})\n","# labels = pd.get_dummies(labels[labels.columns[0]])\n","labels.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[4.97485000e+05 4.42574661e+00 4.42574661e+00 4.42574661e+00\n","  4.42574661e+00]\n"," [8.32075000e+05 4.66232447e+00 4.66232447e+00 4.66232447e+00\n","  4.66232447e+00]\n"," [1.65409000e+05 4.84940970e+00 4.84901116e+00 4.84901117e+00\n","  4.84907991e+00]]\n","[497485. 832075. 165409.]\n","0\n","2\n","1\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   0\n","0  B\n","1  B\n","2  B\n","3  B\n","4  B"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"CmNaA50ZYdy4"},"source":["###Save the Regresion and Label Prediction Files to the drive"]},{"cell_type":"code","metadata":{"id":"lPVce9PHJEwY"},"source":["df.to_csv('multi_pred.csv')\n","!cp multi_pred.csv \"drive/My Drive/\"\n","\n","labels.to_csv('multi_labels.csv')\n","!cp multi_labels.csv \"drive/My Drive/\""],"execution_count":null,"outputs":[]}]}